{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx2y5asdjq57oOqXS7xgDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickfleith/datapipes/blob/main/Evaluation_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with LLM Evaluation Metrics"
      ],
      "metadata": {
        "id": "MvO-7UAzEFeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Exact Match](#scrollTo=VnMbUCEKATj7)\n",
        "\n",
        ">[F1-score, precision, recall](#scrollTo=gV72VzkC3ime)\n",
        "\n",
        "> More coming soon...\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "GnHs0X0IEHmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are numerous ways we can evaluate text generated by LLMs.\n",
        "\n",
        "> **In this notebook we assume we have reference text (gold labels / ground truth) against which we can compare LLM predictions**\n",
        "\n",
        "We'll cover evaluate without references in another notebook.\n",
        "\n",
        "We'll use the `evaluate` library from HuggingFace. Here are all the  metrics provided by this library."
      ],
      "metadata": {
        "id": "ONbwKW4G8DGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aTeR0HDW6kRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8f13d6-f07c-4a7a-e94f-81c69a839f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install evaluate --quiet\n",
        "# You can safely ignore ERROR related to requirements to fsspec==2024.10.0 etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exact Match\n",
        "This is a straightforward metric, although you could be surprised.\n",
        "We used the `evaluate` library from 🤗 HuggingFace.\n",
        "\n",
        "With `evaluate` it generally works as follow:\n",
        "- A list of **references**. The ground truth labels 🙏\n",
        "- A list of **predictions**. The labels of the LLM"
      ],
      "metadata": {
        "id": "VnMbUCEKATj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")"
      ],
      "metadata": {
        "id": "AFFEZHcq8IRL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exactly Exact\n",
        "Here all the words are the same but there is **only 1 perfect match over 4**"
      ],
      "metadata": {
        "id": "4dWX0ycD_t52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"the cat\", \"theater\", \"YELLING\", \"agent007\"] # the ground truth labels\n",
        "predictions = [\"cat\", \"theater\", \"yelling?\", \"agent\"] # what's generated from your LLM\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP9-dAq-_ESm",
        "outputId": "28196fc9-2391-4cee-d69a-7c90727764b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exactly Except\n",
        "- `regexes_to_ignore`: Regex expressions of characters to ignore when calculating the exact matches. Note: these regexes are removed from the input data before the changes based on the options below (e.g. ignore_case,      ignore_punctuation, ignore_numbers) are applied."
      ],
      "metadata": {
        "id": "3cDRqExkD1Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    regexes_to_ignore=[\"the \"]\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69zY59jEDgYe",
        "outputId": "f586f63c-b962-4684-aa92-02315e10fdce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quasy Exactly\n",
        "You also have the following options to ignore:\n",
        "- **`ignore_case`**: Boolean, defaults to False. If true, turns everything to lowercase so that capitalization differences are ignored.\n",
        "- **`ignore_punctuation`**: Boolean, defaults to False. If true, removes all punctuation before comparing predictions and references.\n",
        "- **`ignore_numbers`**: Boolean, defaults to False. If true, removes all punctuation before comparing predictions and references."
      ],
      "metadata": {
        "id": "lNrRPzt3DJHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"the cat\", \"theater\", \"YELLING\", \"agent007\"]\n",
        "predictions = [\"cat\", \"theater\", \"yelling?\", \"agent\"]\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    regexes_to_ignore=[\"the \"],\n",
        "    ignore_case=True,\n",
        "    ignore_punctuation=True,\n",
        "    ignore_numbers=False\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KsKsnYlF69V",
        "outputId": "35ad5b5c-33ab-4d1d-c4cb-d2a93c5c1a1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example with full sentences\n",
        "With exact match you'll probably not compare individual words but full sentences or completions.\n",
        "\n",
        "So here is an example with 2 full sentences."
      ],
      "metadata": {
        "id": "eUMcqTUmGHZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "\n",
        "references = [\n",
        "    \"I like to eat chocolate with my coffee 😀\",\n",
        "    \"Tomorrow, I'll graduate!! So excited\"\n",
        "]\n",
        "\n",
        "predictions = [\n",
        "    \"I like chocolate with coffee\",\n",
        "    \"Tomorrow, I'll graduate! So excited\"\n",
        "]\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    ignore_case=True,\n",
        "    ignore_punctuation=True,\n",
        "    ignore_numbers=True\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re2x1bEpGusG",
        "outputId": "af82fc48-6b34-4ff2-a924-f9b27a4ac3db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1-score, precision, recall\n",
        "You might be already familiar with those metrics from machine learning classification tasks. If not, don't worry.\n",
        "\n",
        "- **F1-score**: It is a balance between the capability to detect probably something and avoiding false detection.\n",
        "\n",
        "- **Precision**: Out of all the answers the model gave, how many were actually correct? (Avoiding false alarms or incorrect guesses.)\n",
        "\n",
        "- **Recall**: Out of all the actually correct answers possible, how many did the model find? (Avoiding missing correct answers.)\n",
        "\n",
        "There are scenarios in which it can be useful. For instance when we use LLM as a classifier (although I wouldn't recommend doing that, instead just use `SetFit`).\n",
        "\n",
        "For instance let's assume you have a dataset of:\n",
        "`True or False: statement?` where you would expect the LLM to answer either True or False. That reduces the evaluation to classification problem, and we can use precision, recall and f1 score."
      ],
      "metadata": {
        "id": "gV72VzkC3ime"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the F1 score?**\n",
        "The F1 is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
        "\n",
        "$$\n",
        "F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "hbi-5JeC5uW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "f1_metric = load(\"f1\")\n",
        "\n",
        "results = f1_metric.compute(\n",
        "    references=[0, 1, 0, 1, 0],\n",
        "    predictions=[0, 0, 1, 1, 0],\n",
        "    # sample_weight=[2, 1, 3, 3, 3]\n",
        ")\n",
        "\n",
        "print(round(results['f1'], 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77bUSdSz3kMF",
        "outputId": "937b4fa4-3c84-4aff-fce0-48ae4b5f4e63"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You could also do multi-class**\n",
        "- That is especially useful for multiple choice question answering (with one correct answer out of several).\n",
        "- In that case you have few options on how to average over each class (like in sklearn f1_score) with `macro`, `micro` or `weighted`."
      ],
      "metadata": {
        "id": "DEribHzv-3k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we have 3 classes: 0, 1, 2\n",
        "predictions = [0, 2, 1, 0, 0, 1]\n",
        "references = [0, 1, 2, 0, 1, 2]\n",
        "\n",
        "results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n",
        "print(round(results['f1'], 2))\n",
        "results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\n",
        "print(round(results['f1'], 2))\n",
        "results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
        "print(round(results['f1'], 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0kvouZ-_SG6",
        "outputId": "1109c456-61c0-43d9-ea0e-fccb8e8c152f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27\n",
            "0.33\n",
            "0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If `average` is set to `None`, the scores for each class are returned."
      ],
      "metadata": {
        "id": "mkBNxUee_5Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = f1_metric.compute(predictions=predictions, references=references, average=None)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpVdQvy4_xyJ",
        "outputId": "4a5be75c-c340-4ff8-a466-34cc7e0d17b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'f1': array([0.8, 0. , 0. ])}\n"
          ]
        }
      ]
    }
  ]
}