{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with LLM Evaluation Metrics"
      ],
      "metadata": {
        "id": "MvO-7UAzEFeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Getting Started with LLM Evaluation Metrics](#scrollTo=MvO-7UAzEFeW)\n",
        "\n",
        ">[Exact Match](#scrollTo=VnMbUCEKATj7)\n",
        "\n",
        ">[F1-score, precision, recall](#scrollTo=gV72VzkC3ime)\n",
        "\n",
        ">[Damerau-Levenshtein Distance](#scrollTo=zklowmjTn-lF)\n",
        "\n",
        ">[Embedding Distance](#scrollTo=pHZXxBPcs0Q4)\n",
        "\n",
        ">[ROUGE and ROUGE-L](#scrollTo=3oXX41hrI6Ho)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "GnHs0X0IEHmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are numerous ways we can evaluate text generated by LLMs.\n",
        "\n",
        "> **In this notebook we assume we have reference text (gold labels / ground truth) against which we can compare LLM predictions**\n",
        "\n",
        "We'll cover evaluate without references in another notebook.\n",
        "\n",
        "We'll use the `evaluate` library from HuggingFace, and also `scikit-learn`"
      ],
      "metadata": {
        "id": "ONbwKW4G8DGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aTeR0HDW6kRD"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate --quiet\n",
        "# You can safely ignore ERROR related to requirements to fsspec==2024.10.0 etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exact Match\n",
        "This is a straightforward metric, although you could be surprised.\n",
        "We used the `evaluate` library from ü§ó HuggingFace.\n",
        "\n",
        "With `evaluate` it generally works as follow:\n",
        "- A list of **references**. The ground truth labels üôè\n",
        "- A list of **predictions**. The labels of the LLM"
      ],
      "metadata": {
        "id": "VnMbUCEKATj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")"
      ],
      "metadata": {
        "id": "AFFEZHcq8IRL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exactly Exact\n",
        "Here all the words are the same but there is **only 1 perfect match over 4**"
      ],
      "metadata": {
        "id": "4dWX0ycD_t52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"the cat\", \"theater\", \"YELLING\", \"agent007\"] # the ground truth labels\n",
        "predictions = [\"cat\", \"theater\", \"yelling?\", \"agent\"] # what's generated from your LLM\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP9-dAq-_ESm",
        "outputId": "1e5a571e-2b03-41ef-fbac-6dd826cce137"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exactly Except\n",
        "- `regexes_to_ignore`: Regex expressions of characters to ignore when calculating the exact matches. Note: these regexes are removed from the input data before the changes based on the options below (e.g. ignore_case,      ignore_punctuation, ignore_numbers) are applied."
      ],
      "metadata": {
        "id": "3cDRqExkD1Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    regexes_to_ignore=[\"the \"]\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69zY59jEDgYe",
        "outputId": "d208ca75-f52c-41b3-d875-8f57e83dc372"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quasy Exactly\n",
        "You also have the following options to ignore:\n",
        "- **`ignore_case`**: Boolean, defaults to False. If true, turns everything to lowercase so that capitalization differences are ignored.\n",
        "- **`ignore_punctuation`**: Boolean, defaults to False. If true, removes all punctuation before comparing predictions and references.\n",
        "- **`ignore_numbers`**: Boolean, defaults to False. If true, removes all punctuation before comparing predictions and references."
      ],
      "metadata": {
        "id": "lNrRPzt3DJHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\"the cat\", \"theater\", \"YELLING\", \"agent007\"]\n",
        "predictions = [\"cat\", \"theater\", \"yelling?\", \"agent\"]\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    regexes_to_ignore=[\"the \"],\n",
        "    ignore_case=True,\n",
        "    ignore_punctuation=True,\n",
        "    ignore_numbers=False\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KsKsnYlF69V",
        "outputId": "bbc48166-50a9-4bdd-e371-6f422e304e70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example with full sentences\n",
        "With exact match you'll probably not compare individual words but full sentences or completions.\n",
        "\n",
        "So here is an example with 2 full sentences."
      ],
      "metadata": {
        "id": "eUMcqTUmGHZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "\n",
        "references = [\n",
        "    \"I like to eat chocolate with my coffee üòÄ\",\n",
        "    \"Tomorrow, I'll graduate!! So excited\"\n",
        "]\n",
        "\n",
        "predictions = [\n",
        "    \"I like chocolate with coffee\",\n",
        "    \"Tomorrow, I'll graduate! So excited\"\n",
        "]\n",
        "\n",
        "results = exact_match_metric.compute(\n",
        "    references=references,\n",
        "    predictions=predictions,\n",
        "    ignore_case=True,\n",
        "    ignore_punctuation=True,\n",
        "    ignore_numbers=True\n",
        ")\n",
        "\n",
        "print(round(results[\"exact_match\"],2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re2x1bEpGusG",
        "outputId": "99423143-15b8-40b6-c3b7-26b14b5dc2e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1-score, precision, recall\n",
        "You might be already familiar with those metrics from machine learning classification tasks. If not, don't worry.\n",
        "\n",
        "- **F1-score**: It is a balance between the capability to detect probably something and avoiding false detection.\n",
        "\n",
        "- **Precision**: Out of all the answers the model gave, how many were actually correct? (Avoiding false alarms or incorrect guesses.)\n",
        "\n",
        "- **Recall**: Out of all the actually correct answers possible, how many did the model find? (Avoiding missing correct answers.)\n",
        "\n",
        "There are scenarios in which it can be useful. For instance when we use LLM as a classifier (although I wouldn't recommend doing that, instead just use `SetFit`).\n",
        "\n",
        "For instance let's assume you have a dataset of:\n",
        "`True or False: statement?` where you would expect the LLM to answer either True or False. That reduces the evaluation to classification problem, and we can use precision, recall and f1 score."
      ],
      "metadata": {
        "id": "gV72VzkC3ime"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the F1 score?**\n",
        "The F1 is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
        "\n",
        "$$\n",
        "F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "hbi-5JeC5uW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example:\n",
        "We will:\n",
        "- Create a dummy function `llm` to simulate a naive LLM which randomly answer `True` or `False` (if you want to use real model, refer to the LLM usage guides).\n",
        "- Make up some statements which are either True or False.\n",
        "- Define **references**, i.e. the if these statements are really True or False, and compare them with the random LLM output."
      ],
      "metadata": {
        "id": "LckdeARAHph5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define a dummy model that randomly predicts true or false\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "def llm(fact_list)->list[bool]:\n",
        "    predictions = [np.random.choice([True, False], ) for item in fact_list]\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "AgujFMQjB32i"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_or_false_statements = [\n",
        "    \"The Earth is a perfect Sphere\",\n",
        "    \"Thomas Pesquet is the First french astronaut on the Moon\",\n",
        "    \"If you could jump straight up more than 100 km, you fall back down to the surface\",\n",
        "    \"Mercury is closer the the Earth than Uranus\",\n",
        "    \"Global Warming is hoax\"\n",
        "]\n",
        "\n",
        "references = [False, False, True, True, False]\n",
        "predictions = llm(fact_list=true_or_false_statements)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkb3ftx8BIIp",
        "outputId": "fd985f24-29ff-4d7d-faad-1ca8936c5ac3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False, True, True, True, True]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "0bR_paAtsU2T"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's convert True/False boolean to integer 0 or 1\n",
        "references = [int(item) for item in references]\n",
        "predictions = [int(pred) for pred in predictions]"
      ],
      "metadata": {
        "id": "DdbMVmFRIaHC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references, predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgM4zIposeub",
        "outputId": "69fd2d7f-28b9-41b1-d63c-75bd86441216"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 1, 1, 0], [0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = f1_score(\n",
        "    y_true=references,\n",
        "    y_pred=predictions,\n",
        "    # sample_weights=[2,1,3,1]\n",
        ")\n",
        "\n",
        "print(round(result, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77bUSdSz3kMF",
        "outputId": "5a88282a-f573-4499-c514-c10b63766dc2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You could also do multi-class**\n",
        "- That is especially useful for multiple choice question answering (with one correct answer out of several).\n",
        "- In that case you have few options on how to average over each class with `macro`, `micro` or `weighted`."
      ],
      "metadata": {
        "id": "DEribHzv-3k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we have 3 classes: 0, 1, 2\n",
        "predictions = [0, 2, 1, 0, 0, 1]\n",
        "references = [0, 1, 2, 0, 1, 2]\n",
        "\n",
        "results = f1_score(y_true=references, y_pred=predictions, average=\"macro\")\n",
        "print(round(results, 2))\n",
        "results = f1_score(y_true=references, y_pred=predictions, average=\"micro\")\n",
        "print(round(results, 2))\n",
        "results = f1_score(y_true=references, y_pred=predictions, average=\"weighted\")\n",
        "print(round(results, 2))"
      ],
      "metadata": {
        "id": "A0kvouZ-_SG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8b4a43-635a-404c-ba11-0855feb210c3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27\n",
            "0.33\n",
            "0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If `average` is set to `None`, the scores for each class are returned."
      ],
      "metadata": {
        "id": "mkBNxUee_5Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = f1_score(y_true=references, y_pred=predictions, average=None)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "FpVdQvy4_xyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b60f4d-f1b9-46ed-ee01-ccde493a4426"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8 0.  0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score"
      ],
      "metadata": {
        "id": "cSGWXGiweRkD"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(\n",
        "    y_true=[0, 1, 0, 1, 0, 1, 0],\n",
        "    y_pred=[0, 0, 1, 1, 0, 1, 1],\n",
        ")"
      ],
      "metadata": {
        "id": "n8sqCwfQeXeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab85aa0-c649-4d5b-ae5a-16a39f114cfd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(\n",
        "    y_true=[0, 1, 0, 1, 0, 1, 0],\n",
        "    y_pred=[0, 0, 1, 1, 0, 1, 1],\n",
        ")"
      ],
      "metadata": {
        "id": "CsO_Fsmvee6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431a3d54-a85f-46ae-f487-f0bb410359f1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Normalization\n",
        "\n",
        "Text normalisation is converting text into a standard format with reduced variability\n",
        "\n",
        "It is not one operations, but a collection of small transformations you could choose to apply of not to your text.\n",
        "\n",
        "ùóòùòÖùóÆùó∫ùóΩùóπùó≤ùòÄ üëáÔ∏è\n",
        "- Lowercasing text: \"Hello\" ‚Üí \"hello\"\n",
        "- Removing punctuation \"Hello, world!\" ‚Üí \"Hello world\"\n",
        "- Removing stopwords: Eliminate common words like \"a,\" \"an,\" and \"the\" that don't add much meaning in some contexts.\n",
        "- Remove extra spaces and normalise them to a single space: \" Hello world \" ‚Üí \"Hello world\"\n",
        "- Reduce words to their base or root forms \"running\" ‚Üí \"run\"\n",
        "- Convert numbers to words (e.g., \"1\" to \"one\") and expand abbreviations (e.g., \"Dr.\" to \"Doctor\")\n",
        "- etc...\n",
        "\n",
        "We normalise to avoid penalisation due to irrelevant variations\n",
        "- normalise your reference text\n",
        "- normalise your prediction\n",
        "Now compare for evaluation\n",
        "\n",
        "But, be careful! Text normalisation strategies can vary based on the problem you're solving. Sometimes, you might want to keep punctuation, or acronyms or something else!\n",
        "\n"
      ],
      "metadata": {
        "id": "aOgVWdvTlcOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"\n",
        "    Normalize a text string by applying several transformations:\n",
        "    1. Convert all characters to lowercase.\n",
        "    2. Remove punctuation marks.\n",
        "    3. Remove articles (\"a\", \"an\", \"the\").\n",
        "    4. Remove extra whitespace.\n",
        "    \"\"\"\n",
        "    ARTICLES_REGEX = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return ARTICLES_REGEX.sub(\" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "metadata": {
        "id": "pokreDsklhnb"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_text(\"The Quick, brown fox jumped over the Lazy DOG!!\")"
      ],
      "metadata": {
        "id": "affSPMQionOR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dbe7a459-5861-4912-92cd-a2ee8a12c482"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quick brown fox jumped over lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Damerau-Levenshtein Distance\n",
        "\n",
        "It measures the **minimum number of single-character edits required to transform one string into another**\n",
        "\n",
        "Available operations of the Levenshtein distance:\n",
        "- character ùó∂ùóªùòÄùó≤ùóøùòÅùó∂ùóºùóª\n",
        "- character ùó±ùó≤ùóπùó≤ùòÅùó∂ùóºùóª\n",
        "- character ùòÄùòÇùóØùòÄùòÅùó∂ùòÅùòÇùòÅùó∂ùóºùóª\n",
        "\n",
        "With Damerau we have one more operation\n",
        "- swapping two adjacent characters (ùòÅùóøùóÆùóªùòÄùóΩùóºùòÄùó∂ùòÅùó∂ùóºùóª)\n",
        "    - detect errors such as typos, where letters are swapped (e.g., \"adn\" ‚Üí \"and\")\n",
        "\n",
        "Why / When it is good ü§ó\n",
        "- ùòÄùó∂ùó∫ùóΩùóπùó≤ ùóÆùóªùó± ùó∂ùóªùòÅùòÇùó∂ùòÅùó∂ùòÉùó≤\n",
        "- great for near match, minor variation\n",
        "- ùó¥ùóøùó≤ùóÆùòÅ ùó≥ùóºùóø ùòÄùóµùóºùóøùòÅ ùóÆùóªùòÄùòÑùó≤ùóø ùòÅùóÆùòÄùó∏ùòÄ\n",
        "- better when precise words are expected\n",
        "- language agnostic\n",
        "- computational efficient\n",
        "\n",
        "Why / When not good ‚ùå\n",
        "- ùòÄùó≤ùó∫ùóÆùóªùòÅùó∂ùó∞ ùòÄùó∂ùó∫ùó∂ùóπùóÆùóøùó∂ùòÅùòÜ ùó∂ùòÄ ùó∂ùóªùó¥ùóºùóøùó≤ùó±. \"Car\" and \"Automobile\" would be considerable different.\n",
        "- ùó¶ùòÅùóøùòÇùó¥ùó¥ùóπùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùóπùóºùóªùó¥ùó≤ùóø ùòÅùó≤ùòÖùòÅ\n",
        "- negation can kill it - \"I will come with you\" is the opposite of \"I will not come with you\" but with still be fairly similar under DL distance."
      ],
      "metadata": {
        "id": "zklowmjTn-lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyxDamerauLevenshtein --quiet"
      ],
      "metadata": {
        "id": "VlduqmTypPGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388b100f-c59d-4c95-cfce-a410a3cdebac"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/62.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyxDamerauLevenshtein (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyxdameraulevenshtein import damerau_levenshtein_distance\n",
        "from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance"
      ],
      "metadata": {
        "id": "eQYrmr3joBhp"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references = [\n",
        "    \"I love chocolate, with my coffee in the morning!!\",\n",
        "    \"hey, the cat is here!\"\n",
        "]\n",
        "\n",
        "predictions = [\n",
        "    \"I eat chocolate with coffee every morning\",\n",
        "    \"hey, your cat is here!\"\n",
        "]"
      ],
      "metadata": {
        "id": "eTeE06EWpQA8"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r, p in zip(references, predictions):\n",
        "\n",
        "    print(\"----- w/o text normalization\\n\")\n",
        "    print(f\"regular: \\t{damerau_levenshtein_distance(r, p)}\")\n",
        "    print(f\"normalized: \\t{round(normalized_damerau_levenshtein_distance(r, p),3)}\\n\")\n",
        "\n",
        "    print(\"----- with text normalization\\n\")\n",
        "    nr, np = normalize_text(r), normalize_text(p)\n",
        "    print(f\"regular: \\t{damerau_levenshtein_distance(nr, np)}\")\n",
        "    print(f\"normalized: \\t{round(normalized_damerau_levenshtein_distance(nr, np),3)}\\n\\n\")"
      ],
      "metadata": {
        "id": "srpMqDEtpU3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776bfeec-0a38-4606-9d13-9d010a86733f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- w/o text normalization\n",
            "\n",
            "regular: \t16\n",
            "normalized: \t0.327\n",
            "\n",
            "----- with text normalization\n",
            "\n",
            "regular: \t12\n",
            "normalized: \t0.286\n",
            "\n",
            "\n",
            "----- w/o text normalization\n",
            "\n",
            "regular: \t4\n",
            "normalized: \t0.182\n",
            "\n",
            "----- with text normalization\n",
            "\n",
            "regular: \t5\n",
            "normalized: \t0.25\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Distance"
      ],
      "metadata": {
        "id": "pHZXxBPcs0Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "def calculate_similarity_score(references, predictions):\n",
        "    # Load a pretrained Sentence Transformer model\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    similarity_score = {\"overall\": 0.0, \"scores\": []}\n",
        "\n",
        "    for refs, preds in zip(references, predictions):\n",
        "        # Encode all references and predictions in batches\n",
        "        ref_embeddings = model.encode(refs, convert_to_tensor=True)\n",
        "        pred_embeddings = model.encode(preds, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate the similarity matrix\n",
        "        similarity_matrix = model.similarity(ref_embeddings, pred_embeddings)\n",
        "\n",
        "        # Find the maximum similarity score for each reference\n",
        "        max_sim = similarity_matrix.max(dim=1).values.max().item()\n",
        "        similarity_score[\"scores\"].append(max_sim)\n",
        "\n",
        "    # Calculate the overall mean score\n",
        "    similarity_score[\"overall\"] = np.mean(similarity_score[\"scores\"])\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage\n",
        "references = [\n",
        "    [\"2024.\", \"two thousand twenty-four\"],\n",
        "    [\"Hello\"],\n",
        "    [\"I like you\"]\n",
        "]\n",
        "predictions = [\n",
        "    [\"Year 2024\"],\n",
        "    [\"Hi\"],\n",
        "    [\"Planet Earth is big\"]\n",
        "]\n",
        "\n",
        "similarity_score = calculate_similarity_score(references, predictions)\n",
        "print(similarity_score)"
      ],
      "metadata": {
        "id": "sSXryfO90Npp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62cb8a3-f5d4-46d3-a612-6632ce7c1348"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'overall': 0.6170853773752848, 'scores': [0.8899925947189331, 0.8071528673171997, 0.15411067008972168]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROUGE and ROUGE-L\n",
        "\n",
        "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.\n",
        "\n",
        "Commonly used to evaluate:\n",
        "- summarisation\n",
        "- translation\n",
        "- question answering\n",
        "\n",
        "> It measures the similarity between a *predicted* text (the output of an LLM for instance), and a *reference* text.\n",
        "\n",
        "It is based on their **Longest Common Subsequence (LCS)** ‚Üí the longest sequence that appears in the same order in both reference and prediction text."
      ],
      "metadata": {
        "id": "3oXX41hrI6Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming:\n",
        "- LCS(X,Y) is the length of the Longest Common Subsequence between the predicted text (ùëã) and the reference text (Y).\n",
        "- \"Length of Prediction\" refers to the number of words in the prediction text.\n",
        "- \"Length of Reference\" refers to the number of words in the reference text.\n",
        "\n",
        "**Precision**\n",
        "- The proportion of the LCS in the prediction text that is also in the reference text.\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{\\text{LCS}(X, Y)}{\\text{Length of Prediction}}\n",
        "$$\n",
        "\n",
        "**Recall**\n",
        "- The proportion of the LCS in the reference text that is also in the predicted text\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{\\text{LCS}(X, Y)}{\\text{Length of Reference}}\n",
        "$$\n",
        "\n",
        "**ROUGE-L (F-measure)**\n",
        "- The harmonic mean of Recall and Precision, providing a balanced score. Probably what you care the most.\n",
        "\n",
        "$$\n",
        "\\text{F}_1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "ùó£ùóøùóºùòÄ\n",
        "- word order captured\n",
        "- efficient, not pre-trained model needed\n",
        "- good for sentence-level overap\n",
        "- good for extractive tasks\n",
        "\n",
        "ùóñùóºùóªùòÄ\n",
        "- fail at semantic similarity and synonyms\n",
        "- tends to favor longer outputs\n",
        "- struggles with creative tasks"
      ],
      "metadata": {
        "id": "eZgv8nKcMq70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score --quiet"
      ],
      "metadata": {
        "id": "0A2Z6lKRJIj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54780dba-c645-4882-9895-717d909fa2c3"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "prediction = \"Quick fox jumped fast over a lazy dog\"\n",
        "\n",
        "# Initialize the scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Compute ROUGE-L\n",
        "scores = scorer.score(reference, prediction)\n",
        "print(\"ROUGE-L Scores:\")\n",
        "print(\"Precision:\", scores['rougeL'].precision)\n",
        "print(\"Recall:\", scores['rougeL'].recall)\n",
        "print(\"F-measure:\", scores['rougeL'].fmeasure)"
      ],
      "metadata": {
        "id": "7yMzmBppL961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b1c15c-107d-4d3f-a934-1b7d77b80fa2"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-L Scores:\n",
            "Precision: 0.75\n",
            "Recall: 0.6666666666666666\n",
            "F-measure: 0.7058823529411765\n"
          ]
        }
      ]
    }
  ]
}