{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBLR71JcdOdSGIurh/W4OD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickfleith/datapipes/blob/main/Structured_Output_with_Google_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Output with Google Gemini\n",
        "In order to build reliable pipelines in which LLMs consistent return output in the same format, we are using a **Structured Output**\n",
        "- This means that we define a blueprint for the output\n",
        "- We pass the 'blueprint' to the LLM\n",
        "- Then the LLM output will confirm to the blueprint.\n",
        "\n",
        "This 'blueprint' in the LLM jargon is often called a \"schema\".\n",
        "\n",
        "**To generate structured outputs with Anthropic, we'll use the library `instructor`**.\n",
        "This will make it very easy to switch between different model providers."
      ],
      "metadata": {
        "id": "w6Gx1TJgGCqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Google Gemini API"
      ],
      "metadata": {
        "id": "LSxQcVBTGNcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install instructor --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvKrsUW2HsqP",
        "outputId": "adc6dfd5-8a60-49f0-d2b0-f44ebc147edf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m317.4/325.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "McHR_WXZFZR3"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from pydantic import BaseModel\n",
        "from enum import Enum\n",
        "import instructor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "Cb5lAMSkF9HQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "qtpunraSF9qI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate JSON\n",
        "When the model is configured to output JSON, it responds to any prompt with JSON-formatted output.\n",
        "\n",
        "You can control the structure of the JSON response by supplying a schema. There are two ways to supply a schema to the model:\n",
        "\n",
        "- As text in the prompt\n",
        "- As a structured schema supplied through model configuration\n",
        "\n",
        "Both approaches work in both Gemini 1.5 Flash and Gemini 1.5 Pro."
      ],
      "metadata": {
        "id": "kfb4Zr6YGRgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class City(str, Enum):\n",
        "    aria = \"Aria\"\n",
        "    kniga = \"Kniga\"\n",
        "    aquabah = \"Aquabah\"\n",
        "    torini = \"Torini\"\n",
        "\n",
        "class Character(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    city: City\n",
        "    job: str\n",
        "    two_sentences_background_story: str\n",
        "    inventory: list[str]"
      ],
      "metadata": {
        "id": "zawtOFFFGBrb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_client = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "pfljCwDpI9oc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = instructor.from_gemini(\n",
        "    client=google_client,\n",
        "    mode=instructor.Mode.GEMINI_JSON,\n",
        ")"
      ],
      "metadata": {
        "id": "Ie8M5Q2YI_EK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note that client.chat.completions.create will also work\n",
        "player = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful table-top RPG gamemaster assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate an Archer character from Kniga for a low-fantasy RPG campaign. Be creative\"},\n",
        "    ],\n",
        "    response_model=Character,\n",
        ")"
      ],
      "metadata": {
        "id": "EcMR-pvUIGN5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "player"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4rfZVhvJGYd",
        "outputId": "6bb15789-8d96-4423-a0fc-ab213dd5a5b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Character(name='Valen', age=24, city=<City.kniga: 'Kniga'>, job='Archer', two_sentences_background_story='Valen grew up in the forests surrounding Kniga, learning to hunt and track from a young age.  His exceptional aim and quiet demeanor led him to join the city guard as an archer, protecting its citizens from both wild beasts and criminal elements.', inventory=['Longbow', 'Quiver of arrows (20)', 'Hunting knife', 'Waterskin', 'Rations (3 days)', 'Leather jerkin'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5Em2APdJb7_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}