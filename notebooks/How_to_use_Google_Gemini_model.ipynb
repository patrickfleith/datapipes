{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to use Google Gemini models with python\n",
        "In this notebook we look into:\n",
        "1. The basics on how to use a Google Gemini model with just a few lines of codes.\n",
        "2. Which settings you can play with to tune the behaviour of the model on your use case."
      ],
      "metadata": {
        "id": "232r2K_5TNs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google API setup\n",
        "\n",
        "In order to use a Google Gemini models, you'll need to create an API key and configure it in your Google Colab Secrets.\n",
        "\n",
        "\n",
        "1. You get your api key from Google AI Studio [here](https://aistudio.google.com/app/apikey)\n",
        "4. Open your Colab secrets (click on the key icon here on the left)\n",
        "3. Give a the name, for instance `GOOGLE_API_KEY`, and past the value in `Value`.\n",
        "4. Toggle `Notebook access` to give access to this specific notebook to the API key.\n",
        "\n",
        "üîë Note that this api key will now be available in your secrets everytime you open or create a new colab notebook. You'll however still need to grant explicit access to each notebook.\n",
        "\n",
        "\n",
        "üí∏ You'll be able to start **free of charge** but you will just be limited in the number of requests you could make to Gemini per minute and per day.\n",
        "- I recommend you start using `gemini-1.5-flash` because you have 15 request per minute and a total of 1500 request per day free, so it's pretty good to strart."
      ],
      "metadata": {
        "id": "bx6eItYrLF43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you are running the notebook outside of Google Colab, uncomment this line to install the Google Generative AI library.\n",
        "#!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "jWMT2QjwUTl8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "czXW-QHkTI4V"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "PZ2gHuv-TM5G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "EXXEQ75HT_Vr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple inference with Gemini model from Google\n",
        "Text generation is very simple.\n",
        "- You need to create a `model` instance.\n",
        "    - This is where you provide a `model_name` üß†\n",
        "- Then you call the `generate_content()` function.\n",
        "    - THis is where you provide the user input query üí¨\n",
        "\n",
        "\n",
        "## Google Models\n",
        "All the model we see are actually multimodals in that we could pass not just text but also Audio, Images, and Videos. But we focus on text for now.\n",
        "\n",
        "I recommend testing models in the following order (from cheaper to more expensive and better).\n",
        "1. `gemini-1.5-flash`: Fast and versatile performance across a diverse variety of tasks\n",
        "    - Input token limit: **1 million !**\n",
        "    - Latest update: September 2024.\n",
        "    - 15 Request per minutes, 1500 requests per day on the free plan\n",
        "2. `gemini-1.5-pro`: For complex reasoning tasks requiring more intelligence.\n",
        "    - Input token limit: **2 millions üò±!!**\n",
        "    - Latest update: October 2024.\n",
        "    - 2 Request per minutes, 50 requests per day on the free plan\n",
        "\n",
        "If you need more capacity you'll have to configure a billing account. And [here](https://ai.google.dev/pricing#1_5flash)\n",
        "\n",
        "üóûÔ∏è  Gemini 2.0 is arriving with more modalities and a thinking mod: `gemini-2.0-flash-exp` with a Knowledge cut-off of August 2024.\n"
      ],
      "metadata": {
        "id": "yksIC4gxM3zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Write a very short poem about an astronaut on the Moon\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_hAnHOZ2TM1h",
        "outputId": "9ce6ddf8-665d-4193-c966-384dc90dbe23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray dust, a giant's leap,\n",
            "Silent, stark, a crater deep.\n",
            "Star-strewn void, a flag unfurls,\n",
            "Earth a jewel, in distant swirls.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash-exp\")\n",
        "response = model.generate_content(\"Write a very short poem about an astronaut on the Moon\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "KGswSjXA2kJD",
        "outputId": "d86136d6-a41f-4c9a-ab7e-4835252d0b0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dusty boots on silver ground,\n",
            "Earth a marble, small and round.\n",
            "Silence screams, a lonely sound,\n",
            "Moon's cold gaze, forever bound.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Parameters\n",
        "\n",
        "[here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is the documentation if you want to dig deeper, but I'll show you the most important ones below.\n",
        "\n",
        "**Having a conversation**\n"
      ],
      "metadata": {
        "id": "jPshoL2NVBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to make an interactive chat?\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "chat = model.start_chat(\n",
        "    history=[\n",
        "        {\"role\": \"user\", \"parts\": \"Hello\"},\n",
        "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
        "    ]\n",
        ")\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
        "print(f\"Model response 1:\\n\")\n",
        "print(response.text)\n",
        "print(f\"Model response 2:\\n\")\n",
        "response = chat.send_message(\"How many paws are in my house?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "QPMBvLf9To69",
        "outputId": "ba85196c-4594-47a6-b437-78f11fddacb6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response 1:\n",
            "\n",
            "That's wonderful!  Do you have any questions about them, or would you like to tell me more about them?  I'd love to hear about your furry friends!\n",
            "\n",
            "Model response 2:\n",
            "\n",
            "If you have two dogs, and each dog has four paws, there are eight paws in your house.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure text generation parameters**\n",
        "\n",
        "Every prompt you send to the model includes [parameters](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) that control how the model generates responses. You can use [GenerationConfig](https://ai.google.dev/api/rest/v1/GenerationConfig) to configure these parameters. If you don't configure the parameters, the model uses default options, which can vary by model."
      ],
      "metadata": {
        "id": "SgZ1gZkwU42N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\n",
        "    \"Tell me a story about a magic backpack.\",\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        candidate_count=1, #  Number of generated responses to return\n",
        "        stop_sequences=[\"x\"], # step of character which stop generation. Rarely used.\n",
        "        max_output_tokens=20, # The maximum number of tokens to include in a response candidate.\n",
        "        temperature=1.0, # Controls the randomness of the output: Betweem 0.0 and 2.0\n",
        "        presence_penalty=1.0,\n",
        "        top_k=10\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8U_tWsD5VNnS",
        "outputId": "4d51f2c9-be15-4d0b-c833-beae9f12cb55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elara lived a life dictated by dust and deadlines. A struggling artist in a bustling city, her\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Like with OpenAI API you can control the following other parameters (for the full details look at [GenerationConfig](https://ai.google.dev/api/rest/v1/GenerationConfig)).\n",
        "- response_schema;\n",
        "- top_p\n",
        "- top_k\n",
        "- presence_penalty\n",
        "- frequence_penalty\n",
        "- logprobs\n",
        "- ..."
      ],
      "metadata": {
        "id": "kHTKxVIPVx0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also count tokens easily\n",
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDUtr0UYXU4S",
        "outputId": "beb2385b-d3a8-420c-e61c-0c4d3d449ed9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 10\n",
            "candidates_token_count: 20\n",
            "total_token_count: 30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can use GenerationConfig to configure these parameters. If you don't configure the parameters, the model uses default options, which can vary by model."
      ],
      "metadata": {
        "id": "Krj96doEUihI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "2G8NbjhMUzbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Write a story about a magic backpack.\", stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.text)\n",
        "    print(\"_\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "LQBfn_t9U1L1",
        "outputId": "3191ec3b-9f39-4592-e115-2a5598f7d6b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El\n",
            "________________________________________________________________________________\n",
            "ara clutched the worn leather straps of the backpack, its faded crimson a stark contrast\n",
            "________________________________________________________________________________\n",
            " to the grey, rain-slicked cobblestones of the market square.\n",
            "________________________________________________________________________________\n",
            " It wasn‚Äôt just any backpack; it was her grandmother‚Äôs, a relic whispered to possess a touch of magic.  Elara, a struggling artist\n",
            "________________________________________________________________________________\n",
            " with a talent for painting but a dearth of luck, had dismissed the tales as folklore until last week.\n",
            "\n",
            "That week, she'd been utterly broke,\n",
            "________________________________________________________________________________\n",
            " staring at a blank canvas and an empty stomach.  In desperation, she‚Äôd opened the ancient backpack, intending to use it for its intended purpose ‚Äì carrying her meager belongings. Instead, she‚Äôd found it overflowing. Not with her\n",
            "________________________________________________________________________________\n",
            " usual sketches and brushes, but with vibrant, sun-ripened oranges, plump, juicy cherries, and a loaf of bread that smelled like a summer meadow.\n",
            "\n",
            "Initially stunned, she‚Äôd eaten her fill, the food tasting like\n",
            "________________________________________________________________________________\n",
            " pure sunshine. The next day, the backpack was replenished with a different bounty: a length of exquisite silk, shimmering like a captured rainbow, and a small pot of the richest, most luminous paint she‚Äôd ever seen.\n",
            "\n",
            "Word spread.  Soon, whispers of the \"miracle backpack\" reached the ears of\n",
            "________________________________________________________________________________\n",
            " Lord Elmsworth, a wealthy patron known for his ruthless business dealings and even more ruthless disregard for artists. He wanted the backpack.  He offered Elara a king's ransom, promising to help her career.  Elara refused.  She knew the backpack wasn‚Äôt about wealth; it was about creating\n",
            "________________________________________________________________________________\n",
            ", about sharing.\n",
            "\n",
            "Lord Elmsworth, used to getting his way, grew angry. He sent his thugs, brute men with scowls etched onto their faces, to steal the backpack.  Elara, small and slight, couldn't fight them.  But as the thugs lunged, she opened her\n",
            "________________________________________________________________________________\n",
            " backpack.\n",
            "\n",
            "This time, instead of food or materials, the backpack spewed forth a torrent of brightly colored butterflies, their wings shimmering with an iridescent dust that blinded the thugs.  They stumbled back, coughing and spluttering, their aggression momentarily forgotten in the swirling chaos of the magical insects.  Elara used the\n",
            "________________________________________________________________________________\n",
            " distraction to flee, the butterflies following her like a protective, fluttering escort.\n",
            "\n",
            "She found refuge in the old artist's quarter, a haven of creativity and camaraderie. There, she shared the backpack‚Äôs bounty with the other artists, using the silk to create stunning tapestries and the luminous paint to craft breathtaking murals\n",
            "________________________________________________________________________________\n",
            ".  The backpack didn't just provide materials; it seemed to inspire, igniting creativity in everyone who came into contact with it.  Her paintings, infused with the magic of the backpack, gained acclaim.  She became known not for wealth, but for her generosity and artistry.\n",
            "\n",
            "Lord Elmsworth never\n",
            "________________________________________________________________________________\n",
            " returned. He had underestimated the true magic of the backpack ‚Äì not the magic of gold and riches, but the magic of sharing, kindness, and the transformative power of art. The backpack, it seemed, chose its owner, not through wealth or power, but through a generous heart. And Elara, its humble guardian\n",
            "________________________________________________________________________________\n",
            ", continued to share its bounty, proving that sometimes, the greatest treasures are not found in gold, but in the magic of giving.\n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}