{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to use an OpenAI Chat model\n",
        "In this notebook we look into:\n",
        "1. The basic on how to use an OpenAI model (chatGPT models) with a few\n",
        "lines of code\n",
        "2. Which settings you can play with to tune the behaviour of the model on your use case."
      ],
      "metadata": {
        "id": "YVocmBocOJH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table of content**\n",
        ">[OpenAI Setup](#scrollTo=7NJMLISB0P3K)\n",
        "\n",
        ">[Simple inference with OpenAI Chat Model](#scrollTo=DyVzBkA2wdHx)\n",
        "\n",
        ">[Advanced Options](#scrollTo=LHvj27ypwf2w)\n",
        "\n",
        ">[Streaming](#scrollTo=NEvt9LQPlWFe)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "OgLl0qPpOG3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Setup"
      ],
      "metadata": {
        "id": "7NJMLISB0P3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "sVQJqN5HWoVx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use OpenAI models, you'll need create an api key and configure it in your Google Colab Secrets.\n",
        "1. Create an openai api key from [here](https://platform.openai.com/settings/organization/api-keys) (you'll need an account on the OpenAI platform, but no need of a ChatGPT subscription).\n",
        "2. Open your Colab secrets (click on the key icon here on the left)\n",
        "3. Give a the name, for instance `OPENAI_API_KEY`, and past the value in `Value`.\n",
        "4. Toggle `Notebook access` to give access to this specific notebook to the API key.\n",
        "\n",
        "\n",
        "ðŸ”‘ Note that this api key will now be available in your secrets everytime you open or create a new colab notebook. You'll however still need to grant explicit access to each notebook.\n",
        "\n",
        "\n",
        "ðŸ’¸ Using an OpenAI model you will get charged! Use a small and cheap model for testing and learning like `gpt-4o-mini` then switch to a better model if needed for more complex tasks."
      ],
      "metadata": {
        "id": "F40lQ7Oh0SPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "yxCsgyDYlgUA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple inference with OpenAI Chat Model\n",
        "Text generation is very simple. You need to create an OpenAI `client` object. You need to call the `.chat.completions.create()` and pass **the two most important arguments**:\n",
        "\n",
        "- ðŸ§  `model` the large language model being used.\n",
        "- ðŸ’¬ `messages` the list of system prompt (optional), user message, and AI assistant responses."
      ],
      "metadata": {
        "id": "DyVzBkA2wdHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Models\n",
        "I recommend testing models in the following order (from cheaper and least capable to most expensive and capable):\n",
        "1. `gpt-4o-mini`: most cost-efficient small model. The model has 128K context and an October 2023 knowledge cutoff.\n",
        "2. `gpt-4o`: most intelligent yet affordable flagship GPT model. The model has 128K context and an October 2023 knowledge cutoff.\n",
        "3. `o1-mini`: faster and cheaper reasoning model particularly good at coding, math, and science.\n",
        "4. `o1`: reasoning model designed to solve hard problems across domains.\n",
        "\n",
        "All the models above have a knowledge cut off at October 2023, and a 128k token limit in the context window (so approx. 100k english words maximum in the input messages).\n",
        "\n",
        "For pricing information: look [here](https://openai.com/api/pricing/)."
      ],
      "metadata": {
        "id": "aowtKKE2TAS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a very short poem about an astronaut on the Moon\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckx2rz0rwaYP",
        "outputId": "e9712cb7-f417-4354-ae92-42aac304f79c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beneath a quiet, silver sky,  \n",
            "An astronaut walks, dreams soaring high.  \n",
            "With each soft step on lunar dust,  \n",
            "He leaves his mark, in stars, he trusts.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Options\n",
        "\n",
        "Here are some more exotic parameters you can use. Their meaning is described right after the code."
      ],
      "metadata": {
        "id": "LHvj27ypwf2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a very short poem about an astronaut on the Moon\"\n",
        "        }\n",
        "    ],\n",
        "    max_completion_tokens=256,\n",
        "    n=3,\n",
        "    temperature=0.7,\n",
        "    frequency_penalty=0.5,\n",
        "    logprobs=True\n",
        ")"
      ],
      "metadata": {
        "id": "NB8qCYOMy7dt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0hAG6ON4aUi",
        "outputId": "aca66b6a-1414-41e8-bbaf-2e8516685df9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beneath the stars, in silence vast,  \n",
            "An astronaut treads where dreams are cast.  \n",
            "Footprints mark the lunar dust,  \n",
            "In weightless wonder, hope and trust.  \n",
            "\n",
            "With Earth a jewel in endless night,  \n",
            "He dances softly in silver light.  \n",
            "A quiet glance at the cosmic sea,  \n",
            "In that stillness, he feels truly free.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[1].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEfwLw9B4bRY",
        "outputId": "9fff9712-c449-4ab3-fb6c-133a6dd60cea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In silver dust, where shadows play,  \n",
            "An astronaut drifts far away.  \n",
            "Beneath the stars, so cold and bright,  \n",
            "He whispers dreams in lunar night.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the full documentation you can look [here](https://platform.openai.com/docs/api-reference/chat/create) but here are my favorite parameters."
      ],
      "metadata": {
        "id": "LG2JTer8Ia1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling OpenAI chat model behaviour\n",
        "You can pass more arguments to control the behaviour of the model\n",
        "- `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
        "- `temperature`: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. OpenAI documentation generally recommend altering this or top_p but not both."
      ],
      "metadata": {
        "id": "Sgp5VG1SWxJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling OpenAI response's additional information\n",
        "- `response_format`: An object specifying the format that the model must output. Compatible with GPT-4o, GPT-4o mini. Typically used to return a JSON\n",
        "\n",
        "- `n`: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."
      ],
      "metadata": {
        "id": "yAvC3E8YW56K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI chat model - Very Advanced ðŸ’ª\n",
        "- `frequency_penalty`: Number between -2.0 and 2.0. Higher increase token diversity.\n",
        "- `presence_penalty`: Number between -2.0 and 2.0. Hier increase token diversity. Inddded, positive values penalize new tokens based on whether they appear in the text so far.\n",
        "- `top_p`: Changes the pool of token to sample from. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
        "\n",
        "The following two are most used for evaluation / audit.\n",
        "- `logprobs` (default to `False`): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
        "- `top_logprobs` (default to `null`): An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is use"
      ],
      "metadata": {
        "id": "fMakj3eMEg5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "Without streaming you have to wait until the full response is created by the model to see it.\n",
        "With **streaming** you see each token as soos as they are generated, like in the ChatGPT interface. Streaming provide a much better user experience.\n",
        "Otherwise, if you don't have user-facing apps, you may not need it."
      ],
      "metadata": {
        "id": "NEvt9LQPlWFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a very short poem about an astronaut on the Moon\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fletbw9tk7AS",
        "outputId": "84b3efb9-e1e1-4489-f7fd-a44d25120547"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In silence deep, where shadows play,  \n",
            "An astronaut treads on silver gray.  \n",
            "Stars like diamonds blink above,  \n",
            "In the vast stillness, whispers of love.  "
          ]
        }
      ]
    }
  ]
}