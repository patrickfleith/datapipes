{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to use an OpenAI Chat model\n",
        "In this notebook we look into:\n",
        "1. The basic on how to use an OpenAI model (chatGPT models) with a few\n",
        "lines of code\n",
        "2. Which settings you can play with to tune the behaviour of the model on your use case."
      ],
      "metadata": {
        "id": "YVocmBocOJH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table of content**\n",
        ">[OpenAI Setup](#scrollTo=7NJMLISB0P3K)\n",
        "\n",
        ">[Simple inference with OpenAI Chat Model](#scrollTo=DyVzBkA2wdHx)\n",
        "\n",
        ">[Advanced Options](#scrollTo=LHvj27ypwf2w)\n",
        "\n",
        ">[Streaming](#scrollTo=NEvt9LQPlWFe)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "OgLl0qPpOG3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Setup"
      ],
      "metadata": {
        "id": "7NJMLISB0P3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "sVQJqN5HWoVx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use OpenAI models, you'll need create an api key and configure it in your Google Colab Secrets.\n",
        "1. Create an openai api key from [here](https://platform.openai.com/settings/organization/api-keys) (you'll need an account on the OpenAI platform, but no need of a ChatGPT subscription).\n",
        "2. Open your Colab secrets (click on the key icon here on the left)\n",
        "3. Give a the name, for instance `OPENAI_API_KEY`, and past the value in `Value`.\n",
        "4. Toggle `Notebook access` to give access to this specific notebook to the API key.\n",
        "\n",
        "\n",
        "ðŸ”‘ Note that this api key will now be available in your secrets everytime you open or create a new colab notebook. You'll however still need to grant explicit access to each notebook.\n",
        "\n",
        "\n",
        "ðŸ’¸ Using an OpenAI model you will get charged! Use a small and cheap model for testing and learning like `gpt-4.1-nano`, or `gpt-4.1-mini` then switch to a better model if needed for more complex tasks."
      ],
      "metadata": {
        "id": "F40lQ7Oh0SPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "yxCsgyDYlgUA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple inference with OpenAI Chat Model\n",
        "Text generation is very simple. You need to create an OpenAI `client` object. You need to call the `.chat.completions.create()` and pass **the two most important arguments**:\n",
        "\n",
        "- ðŸ§  `model` the large language model being used.\n",
        "- ðŸ’¬ `messages` the list of system prompt (optional), user message, and AI assistant responses."
      ],
      "metadata": {
        "id": "DyVzBkA2wdHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Models\n",
        "I recommend testing models in the following order (from cheaper and least capable to most expensive and capable):\n",
        "1. `gpt-4.1-mini`: Balanced for intelligence, speed, and cost.\n",
        "3. `o4-mini`: Faster, more affordable reasoning model\n",
        "2. `gpt-4.1`: Flagship GPT model for complex tasks (intelligent and quite fast)\n",
        "4. `o3`: most powerful reasoning model designed to solve hard problems across domains.\n",
        "\n",
        "All the models above have a knowledge cut off at Jun 01, 2024, and at least 200k tokens limit in the context window (so approx. 150k english words maximum in the input messages). `gpt-4.1` has 1 million token input and 32k output.\n",
        "\n",
        "For more detailed comparison look at this great page: [here](https://platform.openai.com/docs/models/compare)."
      ],
      "metadata": {
        "id": "aowtKKE2TAS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a very short poem about an astronaut on the Moon\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckx2rz0rwaYP",
        "outputId": "f363662f-9546-4b83-9751-ce4d5bd03762"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent steps on silver dust,  \n",
            "Dreams afloat in lunar trust.  \n",
            "Stars below, earthshine above,  \n",
            "A lone soul wrapped in cosmic love.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Options\n",
        "\n",
        "Here are some more exotic parameters you can use. Their meaning is described right after the code."
      ],
      "metadata": {
        "id": "LHvj27ypwf2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a very short poem about an astronaut on the Moon\"\n",
        "        }\n",
        "    ],\n",
        "    max_completion_tokens=256,\n",
        "    n=3,\n",
        "    temperature=0.7,\n",
        "    frequency_penalty=0.5,\n",
        "    logprobs=True\n",
        ")"
      ],
      "metadata": {
        "id": "NB8qCYOMy7dt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0hAG6ON4aUi",
        "outputId": "56194934-8a01-424c-e71f-e8b59986fb6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent steps on silver dust,  \n",
            "A lone heart beats in endless trust.  \n",
            "Stars above, Earth afar,  \n",
            "Dreams unfold where wonders are.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[1].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEfwLw9B4bRY",
        "outputId": "3bab4772-d50a-4009-fe02-886b884c7126"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent steps on silver dust,  \n",
            "Dreams afloat in endless trust.  \n",
            "Stars below and Earth afar,  \n",
            "Moonlit wanderer, cosmic star.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the full documentation you can look [here](https://platform.openai.com/docs/api-reference/chat/create) but here are my favorite parameters."
      ],
      "metadata": {
        "id": "LG2JTer8Ia1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling OpenAI chat model behaviour\n",
        "You can pass more arguments to control the behaviour of the model\n",
        "- `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n",
        "- `temperature`: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. OpenAI documentation generally recommend altering this or top_p but not both."
      ],
      "metadata": {
        "id": "Sgp5VG1SWxJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling OpenAI response's additional information\n",
        "- `response_format`: An object specifying the format that the model must output. Compatible with GPT-4.1, GPT-4.1-mini. Typically used to return a JSON\n",
        "\n",
        "- `n`: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."
      ],
      "metadata": {
        "id": "yAvC3E8YW56K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI chat model - Very Advanced ðŸ’ª\n",
        "- `frequency_penalty`: Number between -2.0 and 2.0. Higher increase token diversity.\n",
        "- `presence_penalty`: Number between -2.0 and 2.0. Hier increase token diversity. Inddded, positive values penalize new tokens based on whether they appear in the text so far.\n",
        "- `top_p`: Changes the pool of token to sample from. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
        "\n",
        "The following two are most used for evaluation / audit.\n",
        "- `logprobs` (default to `False`): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message\n",
        "- `top_logprobs` (default to `null`): An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is use"
      ],
      "metadata": {
        "id": "fMakj3eMEg5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "Without streaming you have to wait until the full response is created by the model to see it.\n",
        "With **streaming** you see each token as soos as they are generated, like in the ChatGPT interface. Streaming provide a much better user experience.\n",
        "Otherwise, if you don't have user-facing apps, you may not need it."
      ],
      "metadata": {
        "id": "NEvt9LQPlWFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a very short poem about an astronaut on the Moon\"}],\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fletbw9tk7AS",
        "outputId": "9a4269af-3b25-45a7-c121-773e9598062e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent steps on dust so gray,  \n",
            "Dreams unfold in lunar sway.  \n",
            "Stars above in endless bloom,  \n",
            "Whispers echoâ€”alone, the Moon."
          ]
        }
      ]
    }
  ]
}