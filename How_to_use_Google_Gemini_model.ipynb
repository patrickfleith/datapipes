{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFlsGUCpL1e4y0rsXV9kNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickfleith/datapipes/blob/main/How_to_use_Google_Gemini_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use Google Gemini models with python\n",
        "In this notebook we look into:\n",
        "1. The basics on how to use a Google Gemini model with just a few lines of codes.\n",
        "2. Which settings you can play with to tune the behaviour of the model on your use case."
      ],
      "metadata": {
        "id": "232r2K_5TNs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google API setup\n",
        "\n",
        "In order to use a Google Gemini models, you'll need to create an API key and configure it in your Google Colab Secrets.\n",
        "\n",
        "\n",
        "1. You get your api key from Google AI Studio [here](https://aistudio.google.com/app/apikey)\n",
        "4. Open your Colab secrets (click on the key icon here on the left)\n",
        "3. Give a the name, for instance `GOOGLE_API_KEY`, and past the value in `Value`.\n",
        "4. Toggle `Notebook access` to give access to this specific notebook to the API key.\n",
        "\n",
        "ðŸ”‘ Note that this api key will now be available in your secrets everytime you open or create a new colab notebook. You'll however still need to grant explicit access to each notebook.\n",
        "\n",
        "\n",
        "ðŸ’¸ You'll be able to start **free of charge** but you will just be limited in the number of requests you could make to Gemini per minute and per day.\n",
        "- I recommend you start using `gemini-1.5-flash` because you have 15 request per minute and a total of 1500 request per day free, so it's pretty good to strart."
      ],
      "metadata": {
        "id": "bx6eItYrLF43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you are running the notebook outside of Google Colab, uncomment this line to install the Google Generative AI library.\n",
        "#!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "jWMT2QjwUTl8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "czXW-QHkTI4V"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "PZ2gHuv-TM5G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "EXXEQ75HT_Vr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple inference with Gemini model from Google\n",
        "Text generation is very simple.\n",
        "- You need to create a `model` instance.\n",
        "    - This is where you provide a `model_name` ðŸ§ \n",
        "- Then you call the `generate_content()` function.\n",
        "    - THis is where you provide the user input query ðŸ’¬\n",
        "\n",
        "\n",
        "## Google Models\n",
        "All the model we see are actually multimodals in that we could pass not just text but also Audio, Images, and Videos. But we focus on text for now.\n",
        "\n",
        "I recommend testing models in the following order (from cheaper to more expensive and better).\n",
        "1. `gemini-1.5-flash`: Fast and versatile performance across a diverse variety of tasks\n",
        "    - Input token limit: **1 million !**\n",
        "    - Latest update: September 2024.\n",
        "    - 15 Request per minutes, 1500 requests per day on the free plan\n",
        "2. `gemini-1.5-pro`: For complex reasoning tasks requiring more intelligence.\n",
        "    - Input token limit: **2 millions ðŸ˜±!!**\n",
        "    - Latest update: October 2024.\n",
        "    - 2 Request per minutes, 50 requests per day on the free plan\n",
        "\n",
        "If you need more capacity you'll have to configure a billing account. And [here](https://ai.google.dev/pricing#1_5flash)\n"
      ],
      "metadata": {
        "id": "yksIC4gxM3zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Write a very short poem about an astronaut on the Moon\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_hAnHOZ2TM1h",
        "outputId": "325b8bfb-91d0-4854-df59-c635b35b6d96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gray dust, a silent sea,\n",
            "One small step, for all to see.\n",
            "Earth a pearl, so far away,\n",
            "Stars ablaze, in lunar day.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Parameters\n",
        "\n",
        "[here](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) is the documentation if you want to dig deeper, but I'll show you the most important ones below.\n",
        "\n",
        "**Having a conversation**\n"
      ],
      "metadata": {
        "id": "jPshoL2NVBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to make an interactive chat?\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "chat = model.start_chat(\n",
        "    history=[\n",
        "        {\"role\": \"user\", \"parts\": \"Hello\"},\n",
        "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
        "    ]\n",
        ")\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
        "print(f\"Model response 1:\\n\")\n",
        "print(response.text)\n",
        "print(f\"Model response 2:\\n\")\n",
        "response = chat.send_message(\"How many paws are in my house?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "QPMBvLf9To69",
        "outputId": "05b750e7-6675-4407-c7d5-1db8b549e53d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response 1:\n",
            "\n",
            "That's wonderful!  Do you have any questions about them, or would you like to tell me more about them?  Perhaps their breeds, names, or favorite things to do?\n",
            "\n",
            "Model response 2:\n",
            "\n",
            "If you have two dogs, and each dog has four paws, then there are 2 * 4 = 8 paws in your house.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure text generation parameters**\n",
        "\n",
        "Every prompt you send to the model includes [parameters](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters) that control how the model generates responses. You can use [GenerationConfig](https://ai.google.dev/api/rest/v1/GenerationConfig) to configure these parameters. If you don't configure the parameters, the model uses default options, which can vary by model."
      ],
      "metadata": {
        "id": "SgZ1gZkwU42N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\n",
        "    \"Tell me a story about a magic backpack.\",\n",
        "    generation_config=genai.types.GenerationConfig(\n",
        "        candidate_count=1, #  Number of generated responses to return\n",
        "        stop_sequences=[\"x\"], # step of character which stop generation. Rarely used.\n",
        "        max_output_tokens=20, # The maximum number of tokens to include in a response candidate.\n",
        "        temperature=1.0, # Controls the randomness of the output: Betweem 0.0 and 2.0\n",
        "        presence_penalty=1.0,\n",
        "        top_k=10\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8U_tWsD5VNnS",
        "outputId": "683e226b-77dd-481e-d54b-87dd61fe6fe9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elara wasn't your typical twelve-year-old.  While other kids obsessed over pop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Like with OpenAI API you can control the following other parameters (for the full details look at [GenerationConfig](https://ai.google.dev/api/rest/v1/GenerationConfig)).\n",
        "- response_schema;\n",
        "- top_p\n",
        "- top_k\n",
        "- presence_penalty\n",
        "- frequence_penalty\n",
        "- logprobs\n",
        "- ..."
      ],
      "metadata": {
        "id": "kHTKxVIPVx0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also count tokens easily\n",
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDUtr0UYXU4S",
        "outputId": "81bdca53-6380-4966-b367-18e844a62157"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 10\n",
            "candidates_token_count: 20\n",
            "total_token_count: 30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can use GenerationConfig to configure these parameters. If you don't configure the parameters, the model uses default options, which can vary by model."
      ],
      "metadata": {
        "id": "Krj96doEUihI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "2G8NbjhMUzbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Write a story about a magic backpack.\", stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.text)\n",
        "    print(\"_\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "LQBfn_t9U1L1",
        "outputId": "b902f03c-bfc4-49ec-a19f-3755623839ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "________________________________________________________________________________\n",
            " old attic smelled of dust and forgotten dreams. Amelia, her nose wrinkled, sifted through dusty boxes\n",
            "________________________________________________________________________________\n",
            ", her fingers brushing against forgotten toys and faded photographs. She was searching for something,\n",
            "________________________________________________________________________________\n",
            " something special, something that whispered of adventure. \n",
            "\n",
            "That's when she found it, tucked behind a chipped porcelain doll: a faded leather backpack, worn and\n",
            "________________________________________________________________________________\n",
            " weathered, with a single, tarnished silver buckle. It felt strangely warm in her hands, as if pulsing with a faint, hidden energy.\n",
            "\n",
            "Amelia\n",
            "________________________________________________________________________________\n",
            " brought the backpack downstairs and, with a mix of trepidation and excitement, unbuckled it. As she did, a shimmering light pulsed from within, casting dancing shadows on the worn wooden floor. When the light subsided, the backpack\n",
            "________________________________________________________________________________\n",
            " was filled with objects, each one more fantastical than the last. \n",
            "\n",
            "There was a compass that pointed not to north, but to the nearest source of adventure. A tiny, silver whistle that, when blown, summoned a talking squirrel who\n",
            "________________________________________________________________________________\n",
            " could translate any animal language. A shimmering silk scarf that, when tied around one's neck, could grant the wearer the ability to fly. \n",
            "\n",
            "Amelia's eyes widened. This wasn't just a backpack; it was a portal to a world of possibilities. Each day, she explored a new facet of\n",
            "________________________________________________________________________________\n",
            " its magic. She flew over the rooftops, laughing as the wind whipped through her hair. She spoke with a wise old owl perched on a mossy branch, learning secrets whispered by the forest. She used the compass to find the hidden grotto where fireflies gathered in a mesmerizing, luminescent ballet. \n",
            "\n",
            "But Amelia wasn'\n",
            "________________________________________________________________________________\n",
            "t the only one who discovered the backpack's magic. Word spread, whispers turning into murmurs, and soon, children from all corners of the town began to gather outside her door, their eyes filled with wonder and a yearning for adventure. Amelia, realizing the power of shared experiences, invited them to join her.\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "They explored ancient ruins, rescued a trapped baby bird, and even helped a grumpy gnome find his lost button. The magic of the backpack, when shared, became even more powerful. It wasn't about individual desires, but about the joy of discovery and the strength of friendship. \n",
            "\n",
            "Years later, Amelia, now\n",
            "________________________________________________________________________________\n",
            " an old woman, passed on the magic backpack to a new generation of children. It was no longer just a bag filled with fantastical objects. It was a symbol of boundless imagination, of shared dreams, and the enduring magic of a world waiting to be explored. The backpack, its leather worn and its silver buckle gleaming\n",
            "________________________________________________________________________________\n",
            " softly, continued to whisper tales of adventure, carrying on its legacy of wonder and delight. \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxo6XQnOU165"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}